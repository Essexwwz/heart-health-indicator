[ERROR] 2021-11-29 19:19:37,117(0) --> [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.internal.Logging.logError(Logging.scala:94): Exception in task 0.0 in stage 1.0 (TID 1)  
java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
	at org.apache.spark.sql.Row.getDouble(Row.scala:270)
	at org.apache.spark.sql.Row.getDouble$(Row.scala:270)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at csvreader$.$anonfun$main$2(csvreader.scala:19)
	at csvreader$.$anonfun$main$2$adapted(csvreader.scala:18)
	at scala.collection.immutable.Range.foreach(Range.scala:156)
	at csvreader$.$anonfun$main$1(csvreader.scala:18)
	at org.apache.spark.sql.execution.MapElementsExec.$anonfun$doExecute$11(objects.scala:300)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:457)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:457)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:457)
	at scala.collection.Iterator.foreach(Iterator.scala:944)
	at scala.collection.Iterator.foreach$(Iterator.scala:944)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1432)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1432)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1432)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1432)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2021-11-29 19:19:37,181(64) --> [task-result-getter-1] org.apache.spark.internal.Logging.logError(Logging.scala:73): Task 0 in stage 1.0 failed 1 times; aborting job  
[ERROR] 2021-11-29 19:20:54,124(1) --> [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.internal.Logging.logError(Logging.scala:94): Exception in task 0.0 in stage 1.0 (TID 1)  
java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
	at org.apache.spark.sql.Row.getDouble(Row.scala:270)
	at org.apache.spark.sql.Row.getDouble$(Row.scala:270)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at csvreader$.$anonfun$main$2(csvreader.scala:19)
	at csvreader$.$anonfun$main$2$adapted(csvreader.scala:18)
	at scala.collection.immutable.Range.foreach(Range.scala:156)
	at csvreader$.$anonfun$main$1(csvreader.scala:18)
	at org.apache.spark.sql.execution.MapElementsExec.$anonfun$doExecute$11(objects.scala:300)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:457)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:457)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:457)
	at scala.collection.Iterator.foreach(Iterator.scala:944)
	at scala.collection.Iterator.foreach$(Iterator.scala:944)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1432)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1432)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1432)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1432)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2021-11-29 19:20:54,173(50) --> [task-result-getter-1] org.apache.spark.internal.Logging.logError(Logging.scala:73): Task 0 in stage 1.0 failed 1 times; aborting job  
[ERROR] 2021-11-29 19:35:56,275(1) --> [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.internal.Logging.logError(Logging.scala:94): Exception in task 0.0 in stage 1.0 (TID 1)  
java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
	at org.apache.spark.sql.Row.getDouble(Row.scala:270)
	at org.apache.spark.sql.Row.getDouble$(Row.scala:270)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at csvreader$.$anonfun$main$2(csvreader.scala:19)
	at csvreader$.$anonfun$main$2$adapted(csvreader.scala:18)
	at scala.collection.immutable.Range.foreach(Range.scala:156)
	at csvreader$.$anonfun$main$1(csvreader.scala:18)
	at org.apache.spark.sql.execution.MapElementsExec.$anonfun$doExecute$11(objects.scala:300)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:457)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:457)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:457)
	at scala.collection.Iterator.foreach(Iterator.scala:944)
	at scala.collection.Iterator.foreach$(Iterator.scala:944)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1432)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1432)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1432)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1432)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2021-11-29 19:35:56,327(53) --> [task-result-getter-1] org.apache.spark.internal.Logging.logError(Logging.scala:73): Task 0 in stage 1.0 failed 1 times; aborting job  
[ERROR] 2021-11-29 19:37:03,780(1) --> [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.internal.Logging.logError(Logging.scala:94): Exception in task 0.0 in stage 1.0 (TID 1)  
java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Double
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:114)
	at org.apache.spark.sql.Row.getDouble(Row.scala:270)
	at org.apache.spark.sql.Row.getDouble$(Row.scala:270)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:166)
	at csvreader$.$anonfun$main$2(csvreader.scala:19)
	at csvreader$.$anonfun$main$2$adapted(csvreader.scala:18)
	at scala.collection.immutable.Range.foreach(Range.scala:156)
	at csvreader$.$anonfun$main$1(csvreader.scala:18)
	at org.apache.spark.sql.execution.MapElementsExec.$anonfun$doExecute$11(objects.scala:300)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:457)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:457)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:457)
	at scala.collection.Iterator.foreach(Iterator.scala:944)
	at scala.collection.Iterator.foreach$(Iterator.scala:944)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1432)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:310)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:308)
	at scala.collection.AbstractIterator.to(Iterator.scala:1432)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:302)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1432)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:289)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:283)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1432)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[ERROR] 2021-11-29 19:37:03,832(53) --> [task-result-getter-1] org.apache.spark.internal.Logging.logError(Logging.scala:73): Task 0 in stage 1.0 failed 1 times; aborting job  
